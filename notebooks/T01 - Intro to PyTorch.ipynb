{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b10504",
   "metadata": {},
   "source": [
    "# Digit Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7bcc3",
   "metadata": {},
   "source": [
    "## 1. Download the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853edec",
   "metadata": {},
   "source": [
    "[Kaggle Page](https://www.kaggle.com/competitions/digit-recognizer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd160d",
   "metadata": {},
   "source": [
    "## 2. Reformat the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9fa4d",
   "metadata": {},
   "source": [
    "We must reformat the data so that we can look at a single image/label pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install matplotlib pandas torch torchvision Pillow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e93545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77bb6a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open csv as pandas dataframe\n",
    "data_dir = \"/Users/amc/Downloads/digit-recognizer\"\n",
    "train_file = os.path.join(data_dir, \"train.csv\")\n",
    "train_df = pd.read_csv(train_file)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29598b",
   "metadata": {},
   "source": [
    "### Split up data into train/val subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f46ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels as an array (could probably be a list instead if you prefer)\n",
    "y = train_df['label']\n",
    "\n",
    "# split pixel info as an array\n",
    "X = train_df.drop(columns=['label'])\n",
    "\n",
    "# split data into train/val/test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "                                                    test_size=0.4, \n",
    "                                                    random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2335b769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train set: 25200\n",
      "length of val set: 16800\n"
     ]
    }
   ],
   "source": [
    "# check lengths of datasets\n",
    "print(f\"length of train set: {len(y_train)}\")\n",
    "print(f\"length of val set: {len(y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d69e1-b577-4861-9497-a8146be43cd5",
   "metadata": {},
   "source": [
    "### Write new dataframes to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e245218-43e3-494f-ac27-9ff2bd55cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/amc/Downloads/digit-recognizer/processed\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# merge back into dataframes\n",
    "column_names = train_df.columns.tolist()\n",
    "train_df = pd.concat([y_train, X_train], axis=1)\n",
    "val_df = pd.concat([y_val, X_val], axis=1)\n",
    "\n",
    "# save to csv files for loading into separate Dataset objects\n",
    "train_df.to_csv(os.path.join(data_dir, \"train.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(data_dir, \"val.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1140534",
   "metadata": {},
   "source": [
    "### Withdraw 1 image+label pair for a given row (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f42466a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify index (row number, starting at 0)\n",
    "idx = 14\n",
    "\n",
    "# save and print label\n",
    "label = train_df.iloc[idx]['label']\n",
    "\n",
    "# save and print image with matplotlib pyplot\n",
    "image = train_df.iloc[idx].to_numpy()  # <- save row as numpy array\n",
    "image = image[1:]  # <- remove label value\n",
    "image = image.reshape(28,28)  # <- reshape numpy array into a 28x28 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1e7c4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 9\n"
     ]
    }
   ],
   "source": [
    "# display image+label pair\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "print(\"label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9a3c5",
   "metadata": {},
   "source": [
    "## 3. Make a custom PyTorch Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac5629",
   "metadata": {},
   "source": [
    "The PyTorch Dataset class automates what we did above.\n",
    "\n",
    "This is a class where you define how to get the image and label for any given index. \n",
    "\n",
    "The index starts at 0 and ends at x-1, where x=number of total samples\n",
    "\n",
    "PyTorch tutorial: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8761099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsDataset(Dataset):\n",
    "    def __init__(self, csv_file: str, transform=None):\n",
    "        \"\"\" Dataset of grayscale number images.\n",
    "        \n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Data transform.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):  \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):  \n",
    "        \n",
    "        # Extract label\n",
    "        label = self.df.iloc[idx]['label']\n",
    "        \n",
    "        # Extract image\n",
    "        image = self.df.iloc[idx].to_numpy(dtype=np.float32)  \n",
    "        image = image[1:]\n",
    "        image = image.reshape(28,28)\n",
    "        \n",
    "        if self.transform: \n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a331f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "train_dataset = DigitsDataset(csv_file=os.path.join(data_dir, \"train.csv\"), \n",
    "                              transform=T.Compose([\n",
    "                                  T.ToTensor()\n",
    "                              ]))\n",
    "val_dataset = DigitsDataset(csv_file=os.path.join(data_dir, \"val.csv\"), \n",
    "                            transform=T.Compose([\n",
    "                                T.ToTensor()\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec82e6",
   "metadata": {},
   "source": [
    "## 4. Make PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834bf0d",
   "metadata": {},
   "source": [
    "The PyTorch DataLoader is a class where you specify batch number and it hands a batch of images for processing through the model. \n",
    "\n",
    "A batch is a set of images to be used for each training iteration. The number of images used for each batch is the batch size. \n",
    "\n",
    "Each training iteration is an \"epoch\", which is where the computer runs through all the images in the dataset once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True) \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1477a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view an image from the dataloader\n",
    "img_batch, label_batch = next(iter(train_dataloader))\n",
    "example_label = label_batch[0]\n",
    "example_img = img_batch[0].squeeze()\n",
    "\n",
    "# show image\n",
    "plt.imshow(example_img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {example_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1dd624",
   "metadata": {},
   "source": [
    "## 5. Make a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b376f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackWhiteModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlackWhiteModel, self).__init__()\n",
    "        \n",
    "        num_classes = 10 \n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9159052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your model with random weights \n",
    "net = BlackWhiteModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d27932",
   "metadata": {},
   "source": [
    "### Run an example image through an untrained model, just for experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of basic usage\n",
    "net.eval()  # put model into eval mode (the is the prediction mode, not the training mode)\n",
    "img_input = img_batch[0].reshape((1, 1, 28, 28))  # make a batch of 1 from our image from dataloader, above\n",
    "output = net(img_input)  # run image through model and get a set of logits, 1 for each class category\n",
    "print(output)  # print logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66875f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply softmax to get probabilities for each class category\n",
    "probs = F.softmax(output[0], dim=0)\n",
    "print(probs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cfdcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return predicted class\n",
    "max_logit, pred_class = output.max(dim=1) # predicts class\n",
    "print(\"predicted class:\", int(pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370599f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = output.argmax()  # also accomplishes the same thing\n",
    "print(\"predicted class:\", int(pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a27fc4",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "Train the model for a a few epochs and see if it can accurately predict the digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad049ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training loop, with only training data but not validation data \n",
    "# (easier to understand at first)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a86d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()  # put model into eval mode (the is the prediction mode, not the training mode)\n",
    "img_input = img_batch[0].reshape((1, 1, 28, 28))  # make a batch of 1 from our image from dataloader, above\n",
    "output = net(img_input)  # run image through model and get a set of logits, 1 for each class category\n",
    "print(output)  # print logits\n",
    "pred_class = output.argmax()  # also accomplishes the same thing\n",
    "print(\"predicted class:\", int(pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9bde7",
   "metadata": {},
   "source": [
    "# THE ALMIGHTY TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fd333",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "min_valid_loss = np.inf  # keeps track of minimum validation loss so we know when to save model\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    \n",
    "    ##### TRAINING #####\n",
    "    train_loss = 0.0\n",
    "    net.train()     # this is default, but nice to look at\n",
    "    for data, labels in train_dataloader:\n",
    "        if torch.cuda.is_available():  # this only runs if you're running this on a GPU \n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        target = net(data)\n",
    "        loss = criterion(target, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    ##### VALIDATION #####\n",
    "    valid_loss = 0.0\n",
    "    net.eval()     # put model in eval mode\n",
    "    for data, labels in val_dataloader:\n",
    "        if torch.cuda.is_available(): # this only runs if you're running this on a GPU \n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        \n",
    "        target = net(data)\n",
    "        loss = criterion(target, labels)\n",
    "        valid_loss = loss.item() * data.size(0)\n",
    "\n",
    "    ##### PRINT STATS & SAVE MODEL #####\n",
    "    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_dataloader)}',\n",
    "          f'\\t\\t Validation Loss: {valid_loss / len(val_dataloader)}')\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict (weights)\n",
    "        model_filename = os.path.join(data_dir, 'bw_model_weights.pth')\n",
    "        torch.save(net.state_dict(), model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50342dec",
   "metadata": {},
   "source": [
    "## Run trained model on a test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b106f8c3-dac4-409c-b020-3e8aae8f96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from test set\n",
    "test_file = \"/Users/amc/Downloads/digit-recognizer/test.csv\"\n",
    "test_df = pd.read_csv(test_file)\n",
    "print(f\"length of test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cac13-2121-4a86-86e2-1d7bcc597653",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56afd8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one image from test set\n",
    "idx = 0\n",
    "test_image = test_df.iloc[idx].to_numpy() \n",
    "test_image = test_image.reshape(28,28)\n",
    "\n",
    "# print image\n",
    "plt.imshow(test_image, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760647dd-92f8-4e72-af6a-e8094959459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image into batched float tensor for model compatibility\n",
    "test_image_tensor = torch.from_numpy(test_image) \n",
    "test_image_tensor = test_image_tensor.float() \n",
    "test_image_tensor = test_image_tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c98b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "trained_net = BlackWhiteModel()\n",
    "model_filename = os.path.join(data_dir, 'bw_model_weights.pth')\n",
    "trained_net.load_state_dict(torch.load(model_filename))\n",
    "\n",
    "# Make prediction (inference)\n",
    "trained_net.eval()  # Set model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations\n",
    "    output = trained_net(test_image_tensor)\n",
    "    pred_class = output.argmax()\n",
    "\n",
    "print(\"Predicted class:\", int(pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58182ee2-40eb-4f2a-937a-c9b5205ffd4b",
   "metadata": {},
   "source": [
    "* * *\n",
    "\n",
    "ðŸš€ Why use `torch.no_grad()`?\n",
    "- Faster Inference\n",
    "- Lower Memory Consumption\n",
    "- No Risk of Accidental Gradient Tracking\n",
    "\n",
    "Always a good practice to use both `.eval()` and `.no_grad()` together for efficient and correct inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b636a37-c6ba-4583-921c-e51d89e60cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

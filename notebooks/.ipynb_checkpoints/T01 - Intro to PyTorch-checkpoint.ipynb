{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b10504",
   "metadata": {},
   "source": [
    "# Digit Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853edec",
   "metadata": {},
   "source": [
    "[Kaggle Page](https://www.kaggle.com/competitions/digit-recognizer/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7bcc3",
   "metadata": {},
   "source": [
    "## 1. Download and preview the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "433301ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label,pixel13,pixel14,pixel15\n",
      "1,0,0,0\n",
      "0,0,0,0\n",
      "1,0,0,0\n",
      "4,0,0,0\n",
      "0,0,0,0\n",
      "0,0,0,0\n",
      "7,0,0,0\n",
      "3,0,0,0\n",
      "0,0,0,\n",
      "3,0,0,0\n",
      "8,0,0,0\n",
      "9,0,0,0\n",
      "1,0,0,0\n",
      "3,0,0,0\n",
      "3,0,0,0\n",
      "1,0,0,0\n",
      "2,0,0,0\n",
      "0,0,0,0\n",
      "7,0,0,0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "data_file=\"/Users/amc/Downloads/data/digits/digits.csv\"\n",
    "cat $data_file | head -n20 | cut -d\",\" -f1,15,16,17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd160d",
   "metadata": {},
   "source": [
    "## 2. Reformat the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9fa4d",
   "metadata": {},
   "source": [
    "We must reformat the data so that we can look at a single image/label pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install matplotlib pandas torch torchvision Pillow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e93545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77bb6a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open csv as pandas dataframe\n",
    "data_dir = \"/Users/amc/Downloads/data/digits\"\n",
    "data_file = os.path.join(data_dir, \"digits.csv\")\n",
    "dataset_df = pd.read_csv(data_file)\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29598b",
   "metadata": {},
   "source": [
    "### Split up data into train/val/test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f46ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split labels as an array (could probably be a list instead if you prefer)\n",
    "y = dataset_df['label']\n",
    "\n",
    "# split pixel info as an array\n",
    "X = dataset_df.drop(columns=['label'])\n",
    "\n",
    "# split data into train/val/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.4, \n",
    "                                                    random_state=23)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, \n",
    "                                                test_size=0.5, \n",
    "                                                random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2335b769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train set: 25200\n",
      "length of val set: 8400\n",
      "length of test set: 8400\n"
     ]
    }
   ],
   "source": [
    "# check lengths of datasets\n",
    "print(f\"length of train set: {len(y_train)}\")\n",
    "print(f\"length of val set: {len(y_val)}\")\n",
    "print(f\"length of test set: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46017277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge back into dataframes\n",
    "column_names = dataset_df.columns.tolist()\n",
    "train_df = pd.concat([y_train, X_train], axis=1)\n",
    "val_df = pd.concat([y_val, X_val], axis=1)\n",
    "test_df = pd.concat([y_test, X_test], axis=1)\n",
    "\n",
    "# save to csv files for loading into separate Dataset objects\n",
    "train_df.to_csv(os.path.join(data_dir, \"train.csv\"), index=False)\n",
    "val_df.to_csv(os.path.join(data_dir, \"val.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(data_dir, \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1140534",
   "metadata": {},
   "source": [
    "### Withdraw 1 image+label pair for a given row (index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f42466a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify index (row number, starting at 0)\n",
    "idx = 14\n",
    "\n",
    "# save and print label\n",
    "label = dataset_df.iloc[idx]['label']\n",
    "\n",
    "# save and print image with matplotlib pyplot\n",
    "image = dataset_df.iloc[idx].to_numpy()  # <- save row as numpy array\n",
    "image = image[1:]  # <- remove label value\n",
    "image = image.reshape(28,28)  # <- reshape numpy array into a 28x28 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1e7c4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 3\n"
     ]
    }
   ],
   "source": [
    "# display image+label pair\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "print(\"label:\", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9a3c5",
   "metadata": {},
   "source": [
    "## 3. Make a PyTorch Dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac5629",
   "metadata": {},
   "source": [
    "The PyTorch dataset object automates what we did above.\n",
    "\n",
    "Essentially, it's a class where you define how to get the image and label for any given index. \n",
    "\n",
    "The index starts at 0 and ends at x-1, where x=number of total samples\n",
    "\n",
    "PyTorch tutorial: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8761099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsDataset(Dataset):\n",
    "    \"\"\"Dataset of grayscale number images.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):  # returns length of dataset\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):  # returns in individual image+label pair\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()  # if the index is a tensor, convert the index to a list so we can use pandas\n",
    "        # save and print label\n",
    "        label = dataset_df.iloc[idx]['label']\n",
    "\n",
    "        # save and print image with matplotlib pyplot\n",
    "        image = dataset_df.iloc[idx].to_numpy(dtype=np.float32)  # <- save row as numpy array\n",
    "        image = image[1:]  # <- remove label value\n",
    "        image = image.reshape(28,28)  # <- reshape numpy array into a 28x28 matrix\n",
    "        #image = Image.fromarray(image)  # <- convert to PIL image, depending which transforms are used\n",
    "        \n",
    "        # apply transforms\n",
    "        if self.transform: \n",
    "            image = self.transform(image)\n",
    "\n",
    "        # returns the image+label pair\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a331f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the datasets\n",
    "train_dataset = DigitsDataset(csv_file=os.path.join(data_dir, \"train.csv\"), \n",
    "                              transform=T.Compose([\n",
    "                                  T.ToTensor()\n",
    "                              ]))\n",
    "val_dataset = DigitsDataset(csv_file=os.path.join(data_dir, \"val.csv\"), \n",
    "                            transform=T.Compose([\n",
    "                                T.ToTensor()\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fec82e6",
   "metadata": {},
   "source": [
    "## 4. Make a PyTorch DataLoader object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834bf0d",
   "metadata": {},
   "source": [
    "The PyTorch DataLoader object is a class where you specify batch number and it hands a batch of images for processing through the neural network. \n",
    "\n",
    "A batch is a set of images to be used for each training iteration. The number of images used for each batch is the batch size. \n",
    "\n",
    "Each training iteration is an \"epoch\", which is where the computer runs through all the images in the dataset once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b172454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=10, shuffle=True, num_workers=0) \n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=True, num_workers=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b1477a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# view an image from the dataloader\n",
    "img_batch, label_batch = next(iter(train_dataloader))\n",
    "example_label = label_batch[0]\n",
    "example_img = img_batch[0].squeeze()\n",
    "\n",
    "# show image\n",
    "plt.imshow(example_img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {example_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1dd624",
   "metadata": {},
   "source": [
    "## 5. Make a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b376f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackWhiteModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BlackWhiteModel, self).__init__()\n",
    "        \n",
    "        num_classes = 10 \n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9159052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your model with random weights \n",
    "net = BlackWhiteModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d27932",
   "metadata": {},
   "source": [
    "### Run an example image through an untrained model, just for experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dea2f70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.0760,  4.4318, -1.2511,  1.1604, -1.9311, -2.8404, -0.2480, -7.6384,\n",
      "          0.4638, -1.8147]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example of basic usage\n",
    "net.eval()  # put model into eval mode (the is the prediction mode, not the training mode)\n",
    "img_input = img_batch[0].reshape((1, 1, 28, 28))  # make a batch of 1 from our image from dataloader, above\n",
    "output = net(img_input)  # run image through model and get a set of logits, 1 for each class category\n",
    "print(output)  # print logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e66875f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.2910e-01, 6.6018e-02, 2.2471e-04, 2.5057e-03, 1.1385e-04, 4.5856e-05,\n",
      "        6.1273e-04, 3.7814e-07, 1.2485e-03, 1.2790e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# apply softmax to get probabilities for each class category\n",
    "probs = F.softmax(output[0], dim=0)\n",
    "print(probs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43cfdcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "# return predicted class\n",
    "max_logit, pred_class = output.max(dim=1) # predicts class\n",
    "print(\"predicted class:\", int(pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "370599f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "pred_class = output.argmax()  # also accomplishes the same thing\n",
    "print(\"predicted class:\", int(pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a27fc4",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "Train the model for a a few epochs and see if it can accurately predict the digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6b7f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fad049ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.725\n",
      "[2,  2000] loss: 0.597\n",
      "[3,  2000] loss: 0.413\n",
      "[4,  2000] loss: 0.334\n",
      "[5,  2000] loss: 0.276\n",
      "[6,  2000] loss: 0.242\n",
      "[7,  2000] loss: 0.215\n",
      "[8,  2000] loss: 0.193\n",
      "[9,  2000] loss: 0.177\n",
      "[10,  2000] loss: 0.159\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# run the training loop, with only training data but not validation data \n",
    "# (easier to understand at first)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a86d159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.5602, -2.3814, -2.3558, -0.2220, -7.6512,  1.3597, -0.6578, -3.2826,\n",
      "          1.6033, -1.9157]], grad_fn=<AddmmBackward0>)\n",
      "predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "net.eval()  # put model into eval mode (the is the prediction mode, not the training mode)\n",
    "img_input = img_batch[0].reshape((1, 1, 28, 28))  # make a batch of 1 from our image from dataloader, above\n",
    "output = net(img_input)  # run image through model and get a set of logits, 1 for each class category\n",
    "print(output)  # print logits\n",
    "pred_class = output.argmax()  # also accomplishes the same thing\n",
    "print(\"predicted class:\", int(pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e9bde7",
   "metadata": {},
   "source": [
    "# THE ALMIGHTY TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "581fd333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t\t Training Loss: 0.14658077941932124 \t\t Validation Loss: 0.000809935498095694\n",
      "Validation Loss Decreased(inf--->0.680346) \t Saving The Model\n",
      "Epoch 2 \t\t Training Loss: 0.13543223879165625 \t\t Validation Loss: 0.00021348977904944193\n",
      "Validation Loss Decreased(0.680346--->0.179331) \t Saving The Model\n",
      "Epoch 3 \t\t Training Loss: 0.12481066901317617 \t\t Validation Loss: 0.0014962961985951378\n",
      "Epoch 4 \t\t Training Loss: 0.11523137534455773 \t\t Validation Loss: 0.00019646314017119862\n",
      "Validation Loss Decreased(0.179331--->0.165029) \t Saving The Model\n",
      "Epoch 5 \t\t Training Loss: 0.10669576913654935 \t\t Validation Loss: 0.0006345000472806749\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "min_valid_loss = np.inf  # keeps track of minimum validation loss so we know when to save model\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    \n",
    "    ##### TRAINING #####\n",
    "    train_loss = 0.0\n",
    "    net.train()     # this is default, but nice to look at\n",
    "    for data, labels in train_dataloader:\n",
    "        if torch.cuda.is_available():  # this only runs if you're running this on a GPU \n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        target = net(data)\n",
    "        loss = criterion(target, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    ##### VALIDATION #####\n",
    "    valid_loss = 0.0\n",
    "    net.eval()     # put model in eval mode\n",
    "    for data, labels in val_dataloader:\n",
    "        if torch.cuda.is_available(): # this only runs if you're running this on a GPU \n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        \n",
    "        target = net(data)\n",
    "        loss = criterion(target, labels)\n",
    "        valid_loss = loss.item() * data.size(0)\n",
    "\n",
    "    ##### PRINT STATS & SAVE MODEL #####\n",
    "    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_dataloader)}',\n",
    "          f'\\t\\t Validation Loss: {valid_loss / len(val_dataloader)}')\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict (weights)\n",
    "        model_filename = os.path.join(data_dir, 'bw_model_weights.pth')\n",
    "        torch.save(net.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50342dec",
   "metadata": {},
   "source": [
    "## Run trained model on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56afd8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print image, as a reminder\n",
    "my_image = img_batch[1].squeeze()\n",
    "plt.imshow(my_image, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79c98b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class: 5\n"
     ]
    }
   ],
   "source": [
    "# load saved model\n",
    "trained_net = BlackWhiteModel()\n",
    "trained_net.load_state_dict(torch.load(model_filename))\n",
    "\n",
    "# make prediction (inference)\n",
    "trained_net.eval() \n",
    "output = trained_net(my_image) \n",
    "pred_class = output.argmax()  # also accomplishes the same thing\n",
    "print(\"predicted class:\", int(pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e7580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
